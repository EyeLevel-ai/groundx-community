{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a6876c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Configuring environment variables, paths, and importing library functionality.\n",
    "\n",
    "This is more complex than what a user would typically do. See the README for installation guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c457bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Load environment and enable local imports\n",
    "import sys, os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the /src directory to sys.path for module imports\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "src_path = os.path.join(repo_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Load environment variables from .env at the repo root\n",
    "dotenv_path = os.path.join(repo_root, \".env\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Importing Project Module\n",
    "from groundx_community.chat_utils.citing import generate_cited_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271fa530",
   "metadata": {},
   "source": [
    "## Sample Query\n",
    "A sample query, simulating retrieved chunks. These are passed to `generate_cited_response`, which performs augmented generation and generates a response with in-text citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e8689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await generate_cited_response(\n",
    "    chunks=[\n",
    "        {\n",
    "            \"text\": (\n",
    "                \"George currently possesses a variety of fruits in his market stall, including three bunches of grapes and a crate of strawberries. \"\n",
    "                \"The grapes are organic, grown locally, and usually sold by the bunch. He typically sells the grapes in quantities of one or more, depending on customer demand. \"\n",
    "                \"The strawberries, on the other hand, are imported and priced differently based on weight rather than quantity.\"\n",
    "            ),\n",
    "            \"uuid\": \"11111111-aaaa-bbbb-cccc-000000000001\",\n",
    "            \"render_name\": \"market_inventory.txt\",\n",
    "            \"source_data\": {\n",
    "                \"url\": \"https://example.com/market_inventory\",\n",
    "                \"filename\": \"market_inventory.txt\",\n",
    "                \"file_type\": \"txt\",\n",
    "                \"document_uuid\": \"doc-001\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"text\": (\n",
    "                \"Natalie recently exchanged €5.00 into U.S. dollars and received exactly $5.45 USD. \"\n",
    "                \"She has expressed interest in buying grapes and strawberries from George. \"\n",
    "                \"However, she also mentioned she wants to keep $1.00 in reserve to buy bread from a nearby bakery later in the day.\"\n",
    "            ),\n",
    "            \"uuid\": \"22222222-bbbb-cccc-dddd-000000000002\",\n",
    "            \"render_name\": \"wallet_and_intentions.txt\",\n",
    "            \"source_data\": {\n",
    "                \"url\": \"https://example.com/wallet\",\n",
    "                \"filename\": \"wallet_and_intentions.txt\",\n",
    "                \"file_type\": \"txt\",\n",
    "                \"document_uuid\": \"doc-002\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"text\": (\n",
    "                \"George stated that he is willing to sell grapes for fifty cents per bunch, but added that if someone buys three bunches, he will offer them for a flat $1.25. \"\n",
    "                \"Strawberries are sold for $3.00 per crate, and George is unwilling to negotiate on that price unless multiple crates are bought. \"\n",
    "                \"All prices are quoted in USD and must be paid in cash.\"\n",
    "            ),\n",
    "            \"uuid\": \"33333333-cccc-dddd-eeee-000000000003\",\n",
    "            \"render_name\": \"pricing_policy.txt\",\n",
    "            \"source_data\": {\n",
    "                \"url\": \"https://example.com/pricing\",\n",
    "                \"filename\": \"pricing_policy.txt\",\n",
    "                \"file_type\": \"txt\",\n",
    "                \"document_uuid\": \"doc-003\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"text\": (\n",
    "                \"A bystander once noted that George used to raise llamas on a small patch of land adjacent to his current fruit farm. \"\n",
    "                \"While this fact has no bearing on his current business dealings, it is often brought up in conversation due to the llama’s quirky behavior.\"\n",
    "            ),\n",
    "            \"uuid\": \"44444444-dddd-eeee-ffff-000000000004\",\n",
    "            \"render_name\": \"llama_memoirs.txt\",\n",
    "            \"source_data\": {\n",
    "                \"url\": \"https://example.com/llamas\",\n",
    "                \"filename\": \"llama_memoirs.txt\",\n",
    "                \"file_type\": \"txt\",\n",
    "                \"document_uuid\": \"doc-004\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    system_prompt=(\n",
    "        \"You are a helpful assistant. Use the provided excerpts to fully analyze and answer the user's question. \"\n",
    "        \"You must rely only on information from the excerpts, and cite every excerpt you used using the $REF: ID$ format. \"\n",
    "        \"If any information is irrelevant, do not cite it or mention it. Think carefully and show clear reasoning.\"\n",
    "        \"provide your answer as a \\\"rationale: \\\" which includes in-text citations, and \\\"final answer: \\\" which is a concise answer to the question. \"\n",
    "        \"the final answer should be separated by the rationale by a paragraph break. The rationale should not include newlines.\"\n",
    "    ),\n",
    "    query=\"Can Natalie afford to buy all of George's grapes and also buy strawberries?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2becc124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rationale: Natalie has $5.45 USD after exchanging her euros, but she wants to keep $1.00 in reserve for bread, leaving her with $4.45 to spend on fruits \\n<InTextCitation chunkId=\"22222222-bbbb-cccc-dddd-000000000002\" renderName=\"wallet_and_intentions.txt\" url=\"https%3A//example.com/wallet\" filename=\"wallet_and_intentions.txt\" file_type=\"txt\" document_uuid=\"doc-002\"></InTextCitation>. George is selling grapes at fifty cents per bunch, or three bunches for $1.25 \\n<InTextCitation chunkId=\"33333333-cccc-dddd-eeee-000000000003\" renderName=\"pricing_policy.txt\" url=\"https%3A//example.com/pricing\" filename=\"pricing_policy.txt\" file_type=\"txt\" document_uuid=\"doc-003\"></InTextCitation>. Since George has three bunches of grapes, Natalie can buy all of them for $1.25. The strawberries are sold for $3.00 per crate \\n<InTextCitation chunkId=\"33333333-cccc-dddd-eeee-000000000003\" renderName=\"pricing_policy.txt\" url=\"https%3A//example.com/pricing\" filename=\"pricing_policy.txt\" file_type=\"txt\" document_uuid=\"doc-003\"></InTextCitation>. Therefore, to buy all the grapes and one crate of strawberries, Natalie would need $1.25 + $3.00 = $4.25. Since she has $4.45 available, she can afford to buy all of George\\'s grapes and one crate of strawberries.\\n\\nFinal answer: Yes, Natalie can afford to buy all of George\\'s grapes and also buy strawberries, as she has $4.45 available and the total cost would be $4.25.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e9c2e",
   "metadata": {},
   "source": [
    "## Rendering the HTML\n",
    "The in-text-citations are injected as `<InTextCitation>'s` within the text document. This line of code does some simple string formatting to convert each in-text citation into an href."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb6743cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .page {\n",
       "            padding: 2rem;\n",
       "            font-family: Arial, sans-serif;\n",
       "            line-height: 1.6;\n",
       "            background-color: #fefefe;\n",
       "            border: 1px solid #ddd;\n",
       "            border-radius: 8px;\n",
       "            max-width: 800px;\n",
       "            margin: 2rem auto;\n",
       "            box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
       "            white-space: normal;\n",
       "        }\n",
       "\n",
       "        intextcitation {\n",
       "            display: inline-block;\n",
       "            background-color: #e0e0ff;\n",
       "            color: #333;\n",
       "            border: 1px solid #aaa;\n",
       "            border-radius: 5px;\n",
       "            padding: 2px 8px;\n",
       "            margin: 0 3px;\n",
       "            font-size: 0.85em;\n",
       "            font-family: monospace;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        intextcitation:hover {\n",
       "            background-color: #d0d0f0;\n",
       "        }\n",
       "\n",
       "        intextcitation::before {\n",
       "            content: attr(renderName);\n",
       "        }\n",
       "\n",
       "        .final-answer {\n",
       "            color: black;\n",
       "        }\n",
       "    </style>\n",
       "\n",
       "    <div class=\"page\">\n",
       "        Rationale: Natalie has $5.45 USD after exchanging her euros, but she wants to keep $1.00 in reserve for bread, leaving her with $4.45 to spend on fruits \n",
       "<InTextCitation chunkId=\"22222222-bbbb-cccc-dddd-000000000002\" renderName=\"wallet_and_intentions.txt\" url=\"https%3A//example.com/wallet\" filename=\"wallet_and_intentions.txt\" file_type=\"txt\" document_uuid=\"doc-002\"></InTextCitation>. George is selling grapes at fifty cents per bunch, or three bunches for $1.25 \n",
       "<InTextCitation chunkId=\"33333333-cccc-dddd-eeee-000000000003\" renderName=\"pricing_policy.txt\" url=\"https%3A//example.com/pricing\" filename=\"pricing_policy.txt\" file_type=\"txt\" document_uuid=\"doc-003\"></InTextCitation>. Since George has three bunches of grapes, Natalie can buy all of them for $1.25. The strawberries are sold for $3.00 per crate \n",
       "<InTextCitation chunkId=\"33333333-cccc-dddd-eeee-000000000003\" renderName=\"pricing_policy.txt\" url=\"https%3A//example.com/pricing\" filename=\"pricing_policy.txt\" file_type=\"txt\" document_uuid=\"doc-003\"></InTextCitation>. Therefore, to buy all the grapes and one crate of strawberries, Natalie would need $1.25 + $3.00 = $4.25. Since she has $4.45 available, she can afford to buy all of George's grapes and one crate of strawberries.\n",
       "\n",
       "Final answer:<span class=\"final-answer\"> Yes, Natalie can afford to buy all of George's grapes and also buy strawberries, as she has $4.45 available and the total cost would be $4.25.</span>\n",
       "    </div>\n",
       "\n",
       "    <script>\n",
       "        setTimeout(() => {\n",
       "            document.querySelectorAll(\"intextcitation\").forEach(el => {\n",
       "                el.addEventListener(\"click\", () => {\n",
       "                    const encodedUrl = el.getAttribute(\"url\");\n",
       "                    if (encodedUrl) {\n",
       "                        const decodedUrl = decodeURIComponent(encodedUrl);\n",
       "                        window.open(decodedUrl, \"_blank\");\n",
       "                    }\n",
       "                });\n",
       "            });\n",
       "        }, 0);\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "def render_citation_with_clickable_custom_tags(html: str) -> str:\n",
    "    # Find the position of \"Final answer\" (case-insensitive)\n",
    "    match = re.search(r\"(final answer:)\", html, re.IGNORECASE)\n",
    "    if match:\n",
    "        start = match.end()\n",
    "        html = html[:start] + '<span class=\"final-answer\">' + html[start:] + '</span>'\n",
    "\n",
    "    styled_html = f\"\"\"\n",
    "    <style>\n",
    "        .page {{\n",
    "            padding: 2rem;\n",
    "            font-family: Arial, sans-serif;\n",
    "            line-height: 1.6;\n",
    "            background-color: #fefefe;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 8px;\n",
    "            max-width: 800px;\n",
    "            margin: 2rem auto;\n",
    "            box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            white-space: normal;\n",
    "        }}\n",
    "\n",
    "        intextcitation {{\n",
    "            display: inline-block;\n",
    "            background-color: #e0e0ff;\n",
    "            color: #333;\n",
    "            border: 1px solid #aaa;\n",
    "            border-radius: 5px;\n",
    "            padding: 2px 8px;\n",
    "            margin: 0 3px;\n",
    "            font-size: 0.85em;\n",
    "            font-family: monospace;\n",
    "            cursor: pointer;\n",
    "        }}\n",
    "\n",
    "        intextcitation:hover {{\n",
    "            background-color: #d0d0f0;\n",
    "        }}\n",
    "\n",
    "        intextcitation::before {{\n",
    "            content: attr(renderName);\n",
    "        }}\n",
    "\n",
    "        .final-answer {{\n",
    "            color: black;\n",
    "        }}\n",
    "    </style>\n",
    "\n",
    "    <div class=\"page\">\n",
    "        {html}\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        setTimeout(() => {{\n",
    "            document.querySelectorAll(\"intextcitation\").forEach(el => {{\n",
    "                el.addEventListener(\"click\", () => {{\n",
    "                    const encodedUrl = el.getAttribute(\"url\");\n",
    "                    if (encodedUrl) {{\n",
    "                        const decodedUrl = decodeURIComponent(encodedUrl);\n",
    "                        window.open(decodedUrl, \"_blank\");\n",
    "                    }}\n",
    "                }});\n",
    "            }});\n",
    "        }}, 0);\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return styled_html\n",
    "\n",
    "# Display the result with clickable citations and final answer highlight\n",
    "display(HTML(render_citation_with_clickable_custom_tags(result)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f42a964",
   "metadata": {},
   "source": [
    "## Integrating with GroundX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa49e2",
   "metadata": {},
   "source": [
    "**Downloading Files:** Downloading a few papers form Arxiv to use as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512af303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading attention_is_all_you_need.pdf (arXiv:1706.03762)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q4/6v5gb2jx49l8k5d3rwfd_x3c0000gn/T/ipykernel_57731/3681963.py:18: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  paper = next(arxiv.Search(id_list=[arxiv_id]).results())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading transformer_xl.pdf (arXiv:1901.02860)...\n",
      "Downloading rope_roformer.pdf (arXiv:2104.09864)...\n",
      "✅ All downloads complete.\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import os\n",
    "\n",
    "# Define target download directory\n",
    "output_dir = os.path.join(\"temp\", \"arxiv_papers\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List of arXiv IDs and desired filenames\n",
    "papers = {\n",
    "    \"1706.03762\": \"attention_is_all_you_need.pdf\",   # Attention is All You Need\n",
    "    \"1901.02860\": \"transformer_xl.pdf\",              # Transformer-XL\n",
    "    \"2104.09864\": \"rope_roformer.pdf\",               # RoPE (RoFormer)\n",
    "}\n",
    "\n",
    "# Download each paper\n",
    "for arxiv_id, filename in papers.items():\n",
    "    print(f\"Downloading {filename} (arXiv:{arxiv_id})...\")\n",
    "    paper = next(arxiv.Search(id_list=[arxiv_id]).results())\n",
    "    paper.download_pdf(dirpath=output_dir, filename=filename)\n",
    "\n",
    "print(\"✅ All downloads complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43339ea",
   "metadata": {},
   "source": [
    "**Uploading Files To Bucket:** Creating a bucket, uploading all downloaded files to a bucket, and waiting for upload to complete\n",
    "\n",
    "I recommend running this block once, then switching the if statement and hard-coding the bucket for future testing iterations.\n",
    "\n",
    "For existing GX users, an existing bucket can be used without uploading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb867ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20266"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Creating a bucket and uploading content to it\n",
    "or hard coding an existing bucket ID\n",
    "\"\"\"\n",
    "from groundx import GroundX\n",
    "\n",
    "client = GroundX(\n",
    "    api_key=os.getenv(\"GROUNDX_API_KEY\"),\n",
    ")\n",
    "\n",
    "#creating and uploading\n",
    "if False:\n",
    "    # Creating a bucket\n",
    "    response = client.buckets.create(\n",
    "        name=\"test_bucket\",\n",
    "    )\n",
    "\n",
    "    bucket_id = response.bucket.bucket_id\n",
    "\n",
    "    print(f\"Created bucket with ID: {bucket_id}\")\n",
    "\n",
    "    \"\"\"Uploading downloaded files to the bucket\n",
    "    \"\"\"\n",
    "    client.ingest_directory(\n",
    "    bucket_id=bucket_id,\n",
    "    path=\"temp/arxiv_papers\",\n",
    "    )\n",
    "\n",
    "#hard coded bucket ID\n",
    "else:\n",
    "    #hard coded bucket ID\n",
    "    bucket_id = 20266\n",
    "bucket_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4e6f8",
   "metadata": {},
   "source": [
    "**Retreiving Content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56281506",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Describe the main differences between the original transformer, the RoFormer, and the Transformer-XL architectures.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409b7c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = client.search.content(\n",
    "    id=bucket_id,\n",
    "    query=query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72378f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "{\"figure_title\":\"RoFormer Position Encoding\",\"figure_number\":\"Figure 1\",\"keywords\":\"transformer, position encoding, attention mechanism, relative position embedding, absolute position embedding, mathematical equations\",\"summary\":\"This figure presents equations related to position encoding in transformer models, focusing on both absolute and relative position embeddings. It illustrates how attention scores are computed and how these scores influence the output of the model. The equations provide a mathematical foundation for understanding the RoFormer architecture and its enhancements over traditional methods.\",\"components\":[\"a_{m,n}\", \"o_m\", \"q_m\", \"k_n\", \"v_n\", \"P_i\", \"p_{i,z}\", \"p_{i,z+1}\"],\"relationships\":[{\"source\":\"query vector q_m\",\"target\":\"key vector k_n\",\"type\":\"attention computation\"},{\"source\":\"attention scores a_{m,n}\",\"target\":\"output o_m\",\"type\":\"weighted sum\"}]}\n",
      "{\"equation\":\"a_{m,n} = exp(q_m^T k_n / √d) / Σ_{j=1}^{N} exp(q_m^T k_j / √d)\",\"relationships\":[{\"source\":\"q_m\",\"target\":\"k_n\",\"type\":\"dot product\"},{\"source\":\"a_{m,n}\",\"target\":\"o_m\",\"type\":\"output computation\"}]}\n",
      "{\"equation\":\"o_m = Σ_{n=1}^{N} a_{m,n} v_n\",\"relationships\":[{\"source\":\"a_{m,n}\",\"target\":\"v_n\",\"type\":\"weighted sum\"},{\"source\":\"o_m\",\"target\":\"output\",\"type\":\"final computation\"}]}\n",
      "{\"equation\":\"f_t:t∈{q,k,v}(x_i, i) := W_t:t∈{q,k,v}(x_i + P_i)\",\"relationships\":[{\"source\":\"x_i\",\"target\":\"P_i\",\"type\":\"addition\"},{\"source\":\"W_t\",\"target\":\"context representation\",\"type\":\"transformation\"}]}\n",
      "{\"equation\":\"P_{i,2t} = sin(k / 10000^{2t / d}), P_{i,2t+1} = cos(k / 10000^{2t / d})\",\"relationships\":[{\"source\":\"k\",\"target\":\"P_{i,2t}\",\"type\":\"sinusoidal encoding\"},{\"source\":\"k\",\"target\":\"P_{i,2t+1}\",\"type\":\"sinusoidal encoding\"}]}\n",
      "{\"equation\":\"f_q(x_m) := W_q x_m, f_k(x_n, n) := W_k(x_n + p_r), f_v(x_n, n) := W_v(x_n + p_r)\",\"relationships\":[{\"source\":\"x_m\",\"target\":\"W_q\",\"type\":\"query transformation\"},{\"source\":\"x_n\",\"target\":\"p_r\",\"type\":\"relative position addition\"},{\"source\":\"W_k\",\"target\":\"key transformation\"},{\"source\":\"W_v\",\"target\":\"value transformation\"}]}\n",
      "{\"equation\":\"q_m k_n = x_m^T W_q^T W_k x_n + x_m^T W_q^T W_k p_{m-n} + p_m^T W_q^T W_k x_n + p_m^T W_q^T W_k p_{m-n}\",\"relationships\":[{\"source\":\"x_m\",\"target\":\"W_q^T W_k x_n\",\"type\":\"content-based computation\"},{\"source\":\"x_m\",\"target\":\"W_q^T W_k p_{m-n}\",\"type\":\"relative position computation\"},{\"source\":\"p_m\",\"target\":\"W_q^T W_k x_n\",\"type\":\"absolute position computation\"},{\"source\":\"p_m\",\"target\":\"W_q^T W_k p_{m-n}\",\"type\":\"relative position computation\"}]}\n",
      "{\"equation\":\"q_m k_n = x_m^T W_q^T W_k x_n + x_m^T W_q^T W_k p_{m-n} + u^T W_q^T W_k x_n + v^T W_q^T W_k p_{m-n}\",\"relationships\":[{\"source\":\"x_m\",\"target\":\"W_q^T W_k x_n\",\"type\":\"content-based computation\"},{\"source\":\"x_m\",\"target\":\"W_q^T W_k p_{m-n}\",\"type\":\"relative position computation\"},{\"source\":\"u\",\"target\":\"W_q^T W_k x_n\",\"type\":\"trainable vector computation\"},{\"source\":\"v\",\"target\":\"W_q^T W_k p_{m-n}\",\"type\":\"trainable vector computation\"}]}\n",
      "{\"equation\":\"q_m k_n = x_m^T W_q^T W_k x_n + b_{i,j}\",\"relationships\":[{\"source\":\"x_m\",\"target\":\"W_q^T W_k x_n\",\"type\":\"content-based computation\"},{\"source\":\"b_{i,j}\",\"target\":\"attention weights\",\"type\":\"bias addition\"}]}\n",
      "{\"equation\":\"q_m k_n = x_m^T W_q^T W_k x_n + P_m^T U^T U_k P_n + b_{i,j}\",\"relationships\":[{\"source\":\"x_m\",\"target\":\"W_q^T W_k x_n\",\"type\":\"content-based computation\"},{\"source\":\"P_m\",\"target\":\"U^T U_k P_n\",\"type\":\"projection matrix computation\"},{\"source\":\"b_{i,j}\",\"target\":\"attention weights\",\"type\":\"bias addition\"}]}\n",
      "\n",
      "The image contains mathematical equations related to position encoding in transformer models. The first equation computes attention scores \\(a_{m,n}\\) as the normalized exponential of the dot product between query vector \\(q_m\\) and key vector \\(k_n\\), divided by the square root of the dimension \\(d\\). The second equation calculates the output \\(o_m\\) as the weighted sum of values \\(v_n\\) based on the attention scores \\(a_{m,n}\\). Subsequent equations explore absolute and relative position embeddings. Absolute position embedding adds position vectors \\(P_i\\) to token representations, with sinusoidal functions used to generate these vectors. Relative position embedding incorporates relative distances between tokens, represented by \\(p_r\\), into the attention mechanism. Advanced equations decompose the attention computation into content-based and position-based components, introducing trainable vectors \\(u\\) and \\(v\\) for greater flexibility. Later refinements include bias terms \\(b_{i,j}\\) and projection matrices to model relationships between positions and tokens. These equations collectively describe the mathematical foundation of the RoFormer architecture, emphasizing enhancements over traditional position encoding methods.\n",
      "-----------\n",
      "{\"table_title\":\"The proposed RoFormer gives better BLEU scores compared to its baseline alternative Vaswani et al. [2017] on the WMT 2014 English-to-German translation task Bojar et al. [2014].\",\"table_number\":1,\"keywords\":\"RoFormer, BLEU scores, Transformer-based model, WMT 2014, English-German translation, Vaswani et al. [2017], Bojar et al. [2014], machine translation, sequence-to-sequence tasks\",\"summary\":\"The following table contains BLEU scores comparing the performance of the RoFormer model and the baseline Transformer-based model on the WMT 2014 English-to-German translation task. The table demonstrates that RoFormer achieves a higher BLEU score of 27.5 compared to the baseline model's 27.3, indicating improved performance in sequence-to-sequence translation tasks.\"}\n",
      "{\"Model\":\"Transformer-based Vaswani et al. [2017]\",\"BLEU\":27.3}\n",
      "{\"Model\":\"RoFormer\",\"BLEU\":27.5}\n",
      "\n",
      "The table compares the BLEU scores of two models: the Transformer-based model referenced by Vaswani et al. [2017] and the RoFormer model. The Transformer-based model achieves a BLEU score of 27.3, while the RoFormer model achieves a higher BLEU score of 27.5. This indicates that RoFormer outperforms the baseline model in the WMT 2014 English-to-German translation task.\n",
      "-----------\n",
      "{\"table_title\":\"Table 3\",\"headers\":[\"Model\",\"Tokenization level\",\"Position embedding\"],\"summary\":\"The following table contains a cross-comparison of pre-trained transformer-based models on Chinese data, focusing on tokenization levels and position embedding methods.\",\"keywords\":\"RoFormer, BERT, WoBERT, NEZHA, tokenization, position embedding, absolute embedding, relative embedding, ROPE\"}\n",
      "{\"Model\":\"BERT Devlin et al. 2019\",\"Tokenization level\":\"char\",\"Position embedding\":\"abs.\"}\n",
      "{\"Model\":\"WoBERT Su 2020\",\"Tokenization level\":\"word\",\"Position embedding\":\"abs.\"}\n",
      "{\"Model\":\"NEZHA Wei et al. 2019\",\"Tokenization level\":\"char\",\"Position embedding\":\"rel.\"}\n",
      "{\"Model\":\"RoFormer\",\"Tokenization level\":\"word\",\"Position embedding\":\"RoPE\"}\n",
      "\n",
      "The table compares four pre-trained transformer-based models on Chinese data: BERT, WoBERT, NEZHA, and RoFormer. Each model is evaluated based on its tokenization level and position embedding method. BERT uses character-level tokenization with absolute position embedding, while WoBERT employs word-level tokenization with absolute position embedding. NEZHA utilizes character-level tokenization with relative position embedding, and RoFormer adopts word-level tokenization with the proposed Rotary Position Embedding (RoPE). This comparison highlights the differences in tokenization and embedding approaches among the models.\n",
      "-----------\n",
      "{\n",
      "  \"arXiv\": \"arXiv:2104.09864v5 [cs.CL]\",\n",
      "  \"title\": \"ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING\",\n",
      "  \"abstract\": \"Position encoding has recently shown effectiveness in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (ROPE) to effectively leverage the positional information. Specifically, the proposed ROPE encodes the absolute position with a rotation matrix and simultaneously incorporates the explicit relative position dependency in the self-attention formulation. Notably, ROPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently outperforms its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Hugging Face: https://huggingface.co/docs/transformers/model_doc/roformer.\",\n",
      "  \"keywords\": \"Pre-trained Language Models, Position Information Encoding, Pre-training, Natural Language Processing.\",\n",
      "  \"introduction\": \"1 Introduction\"\n",
      "}\n",
      "{\n",
      "  \"text\": \"The sequential order of words is of great value to natural language understanding. Recurrent neural networks (RNNs) based models encode tokens' order by recursively computing a hidden state along the time dimension. Convolutional neural networks (CNNs) based models, as discussed by Gehring et al. [2017], were typically considered position-agnostic, but recent work by Islam et al. [2020] has shown that the commonly used padding operation can implicitly learn position information. Recently, the pre-trained language models (PLMs), which were built upon the transformer architecture introduced by Vaswani et al. [2017], have achieved state-of-the-art performance in various natural language processing (NLP) tasks, including context representation learning as demonstrated by Devlin et al. [2019], machine translation as shown by Vaswani et al. [2017], and language modeling as presented by Radford et al. [2019], to name a few. Unlike RNNs and CNNs-based models, PLMs utilize the self-attention mechanism to semantically capture the contextual representation of a given corpus. As a consequence, PLMs achieve a significant improvement in terms of parallelization over RNNs and improve the modeling ability of longer intra-token relations compared to CNNs.¹ A stack of multiple CNN layers can also capture longer intra-token relations; here, we only consider a single layer setting. RoFormer\"\n",
      "}\n",
      "-----------\n",
      "{\n",
      "  \"4.1 Machine Translation\": \"We first demonstrate the performance of RoFormer on sequence-to-sequence language translation tasks.\",\n",
      "  \"4.1.1 Experimental Settings\": \"We choose the standard WMT 2014 English-German dataset (Bojar et al. [2014]), which consists of approximately 4.5 million sentence pairs. We compare our results to the transformer-based baseline alternative (Vaswani et al. [2017]).\",\n",
      "  \"4.1.2 Implementation details\": \"We carry out some modifications on the self-attention layer of the baseline model (Vaswani et al. [2017]) to enable ROPE to enhance its learning process. We replicate the setup for English-to-German translation with a vocabulary of 37,000 based on a joint source and target byte pair encoding (BPE) (Sennrich et al. [2015]). During the evaluation, a single model is obtained by averaging the last 5 checkpoints. The result uses beam search with a beam size of 4 and a length penalty of 0.6. We implement the experiment in PyTorch using the fairseq toolkit (MIT License) (Ott et al. [2019]). Our model is optimized with the Adam optimizer using β₁ = 0.9, β₂ = 0.98, with the learning rate increased linearly from 1e-7 to 5e-4 and then decayed proportionally to the inverse square root of the step number. Label smoothing with a value of 0.1 is also adopted. We report the BLEU (Papineni et al. [2002]) score on the test set as the final metric.\",\n",
      "  \"4.1.3 Results\": \"We train the baseline model and our RoFormer under the same settings and report the results in Table 1. As can be seen, our model gives better BLEU scores compared to the baseline Transformer.\",\n",
      "  \"4.2 Pre-training Language Modeling\": \"The second experiment is to validate the performance of our proposal in terms of learning contextual representations. To achieve this, we replace the original sinusoidal position encoding of BERT with our ROPE during the pre-training step.\",\n",
      "  \"4.2.1 Experimental Settings\": \"\"\n",
      "}\n",
      "{\n",
      "  \"text\": \"We use the BookCorpus Zhu et al. [2015] and the Wikipedia Corpus Foundation [2021] from the Hugging Face Datasets library (Apache License 2.0) for pre-training. The corpus is further split into training and validation sets at an 80:20 ratio. We use the masked language-modeling (MLM) loss values of the training process as an evaluation metric. The well-known BERT model, as introduced by Devlin et al. [2019], is adopted as our baseline model. Note that we use bert-base-uncased in our experiments.\\n\\n4.2.2 Implementation details\\nFor RoFormer, we replace the sinusoidal position encoding in the self-attention block of the baseline model with our proposed Rotary Position Embedding (ROPE) and realize self-attention according to Equation (16). We train both BERT and RoFormer with a batch size of 64 and a maximum sequence length of 512 for 100,000 steps. AdamW, as described by Loshchilov and Hutter [2017], is used as the optimizer with a learning rate of 1e-5.\\n\\n4.2.3 Results\\nThe MLM loss during pre-training is shown in the left plot of Figure (3). Compared to the vanilla BERT, RoFormer experiences faster convergence.\"\n",
      "}\n",
      "-----------\n",
      "{\"table_title\":\"Rotary Matrix Representation in RoFormer\",\"keywords\":\"Rotary Position Embedding, RoFormer, rotary matrix, position encoding, self-attention, natural language processing, transformer models\",\"summary\":\"The following table contains the mathematical representation of the rotary matrix used in the RoFormer model for encoding position information. It illustrates how the d-dimensional space is divided into subspaces and combined using the linearity of the inner product. The table includes formulas for the rotary matrix components, showcasing the cosine and sine transformations applied to encode positional data. This approach highlights the multiplicative nature of RoPE, which incorporates relative position information through matrix operations.\",\"trends\":\"The rotary matrix ensures stability during encoding and improves computational efficiency by leveraging sparsity in high-dimensional spaces.\"}\n",
      "{\"row_1\":{\"cos_mθ1\":\"cos(mθ1)\",\"sin_mθ1\":\"sin(mθ1)\",\"negative_sin_mθ1\":\"-sin(mθ1)\",\"cos_mθ1\":\"cos(mθ1)\",\"zeros\":\"0\"}}\n",
      "{\"row_2\":{\"cos_mθ2\":\"cos(mθ2)\",\"sin_mθ2\":\"sin(mθ2)\",\"negative_sin_mθ2\":\"-sin(mθ2)\",\"cos_mθ2\":\"cos(mθ2)\",\"zeros\":\"0\"}}\n",
      "{\"row_3\":{\"ellipsis\":\"...\",\"ellipsis\":\"...\",\"ellipsis\":\"...\",\"ellipsis\":\"...\",\"ellipsis\":\"...\"}}\n",
      "{\"row_4\":{\"cos_mθ_d/2\":\"cos(mθ_d/2)\",\"sin_mθ_d/2\":\"sin(mθ_d/2)\",\"negative_sin_mθ_d/2\":\"-sin(mθ_d/2)\",\"cos_mθ_d/2\":\"cos(mθ_d/2)\",\"zeros\":\"0\"}}\n",
      "\n",
      "The rotary matrix is defined as a structured combination of cosine and sine transformations applied to positional data. Each row represents a subspace of the d-dimensional space, with alternating cosine and sine values to encode positional information. The first row includes values for the first dimension (mθ1), while subsequent rows extend this encoding to higher dimensions (mθ2, ..., mθ_d/2). The ellipsis indicates the continuation of the pattern for intermediate dimensions. This matrix is used to ensure stability and computational efficiency in the RoFormer model's self-attention mechanism.\n",
      "-----------\n",
      "{\n",
      "  \"4.5.5 Limitations of the work\": \"Although we provide theoretical groundings as well as promising experimental justifications, our method is limited by the following facts:\\n• Despite the fact that we mathematically format the relative position relations as rotations under 2D sub-spaces, there is a lack of thorough explanations on why it converges faster than baseline models that incorporate other position encoding strategies.\\n• Although we have proved that our model has the favorable property of long-term decay for inter-token products, as discussed in Section 3.3, which is similar to the existing position encoding mechanisms, our model shows superior performance on long texts compared to peer models; however, we have not come up with a faithful explanation for this phenomenon.\\n• Our proposed RoFormer is built upon the Transformer-based infrastructure, which requires significant hardware resources for pre-training purposes.\\n\\n5 Conclusions\\nIn this work, we proposed a new position embedding method that incorporates explicit relative position dependency in self-attention to enhance the performance of transformer architectures. Our theoretical analysis indicates that relative position can be naturally formulated using vector production in self-attention, with absolute position information being encoded through a rotation matrix. In addition, we mathematically illustrated the advantageous properties of the proposed method when applied to the Transformer. Finally, experiments on both English and Chinese benchmark datasets demonstrate that our method encourages faster convergence in pre-training. The experimental results also show that our proposed RoFormer can achieve better performance on long text tasks.\\n\\nReferences\"\n",
      "}\n",
      "{\n",
      "  \"references\": [\n",
      "    {\n",
      "      \"authors\": \"Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin\",\n",
      "      \"title\": \"Convolutional sequence to sequence learning\",\n",
      "      \"conference\": \"International Conference on Machine Learning\",\n",
      "      \"pages\": \"1243-1252\",\n",
      "      \"publisher\": \"PMLR\",\n",
      "      \"year\": 2017\n",
      "    },\n",
      "    {\n",
      "      \"authors\": \"Md. Amirul Islam, Sen Jia, and Neil D. B. Bruce\",\n",
      "      \"title\": \"How much position information do convolutional neural networks encode?\",\n",
      "      \"source\": \"ArXiv\",\n",
      "      \"identifier\": \"abs/2001.08248\",\n",
      "      \"year\": 2020\n",
      "    },\n",
      "    {\n",
      "      \"authors\": \"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin\",\n",
      "      \"title\": \"Attention is all you need\",\n",
      "      \"editors\": \"I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\",\n",
      "      \"conference\": \"Advances in Neural Information Processing Systems\",\n",
      "      \"volume\": 30,\n",
      "      \"publisher\": \"Curran Associates, Inc.\",\n",
      "      \"year\": 2017,\n",
      "      \"url\": \"https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa - Paper.pdf\"\n",
      "    },\n",
      "    {\n",
      "      \"authors\": \"J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova\",\n",
      "      \"title\": \"BERT: Pre-training of deep bidirectional transformers for language understanding\",\n",
      "      \"conference\": \"NAACL-HLT\",\n",
      "      \"year\": 2019\n",
      "    },\n",
      "    {\n",
      "      \"authors\": \"A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever\",\n",
      "      \"title\": \"Language models are unsupervised multitask learners\",\n",
      "      \"year\": 2019\n",
      "    },\n",
      "    {\n",
      "      \"authors\": \"Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar\",\n",
      "      \"title\": \"Are transformers universal approximators of sequence-to-sequence functions?\",\n",
      "      \"conference\": \"International Conference on Learning Representations\",\n",
      "      \"year\": 2020,\n",
      "      \"url\": \"https://openreview.net/forum?id=ByxRMONtvr\"\n",
      "    },\n",
      "    {\n",
      "      \"authors\": \"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut\",\n",
      "      \"title\": \"ALBERT: A lite BERT for self-supervised learning of language representations\",\n",
      "      \"conference\": \"International Conference on Learning Representations\",\n",
      "      \"year\": 2020,\n",
      "      \"url\": \"https://openreview.net/forum?id=H1eA7AEtvS\"\n",
      "    },\n",
      "    {\n",
      "      \"authors\": \"Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning\",\n",
      "      \"title\": \"ELECTRA: Pre-training text encoders as discriminators rather than generators\",\n",
      "      \"conference\": \"ICLR\",\n",
      "      \"year\": 2020,\n",
      "      \"url\": \"https://openreview.net/pdf?id=r1xMH1BtvB\"\n",
      "    },\n",
      "    {\n",
      "      \"authors\": \"A. Radford and Karthik Narasimhan\",\n",
      "      \"title\": \"Improving language understanding by generative pre-training\",\n",
      "      \"year\": 2018\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "-----------\n",
      "{\"summary\":\"Table 3: Comparison with state-of-the-art results on text8.\",\"keywords\":\"Transformer-XL, state-of-the-art, language models, text8, performance, parameters, perplexity, neural architecture, model comparison, document purpose, 2019 publication.\"}\n",
      "{\"Model\":\"Shazeer et al. (2014) - Sparse Non-Negative\",\"#Param\":\"33B\",\"PPL\":\"52.9\"}\n",
      "{\"Model\":\"Chelba et al. (2013) - RNN-1024 + 9 Gram\",\"#Param\":\"20B\",\"PPL\":\"51.3\"}\n",
      "{\"Model\":\"Kuchaiev and Ginsburg (2017) - G-LSTM-2\",\"#Param\":\"-\",\"PPL\":\"36.0\"}\n",
      "{\"Model\":\"Dauphin et al. (2016) - GCNN-14 bottleneck\",\"#Param\":\"-\",\"PPL\":\"31.9\"}\n",
      "{\"Model\":\"Jozefowicz et al. (2016) - LSTM\",\"#Param\":\"1.8B\",\"PPL\":\"30.6\"}\n",
      "{\"Model\":\"Jozefowicz et al. (2016) - LSTM + CNN Input\",\"#Param\":\"1.04B\",\"PPL\":\"30.0\"}\n",
      "{\"Model\":\"Shazeer et al. (2017) - Low-Budget MoE\",\"#Param\":\"~5B\",\"PPL\":\"34.1\"}\n",
      "{\"Model\":\"Shazeer et al. (2017) - High-Budget MoE\",\"#Param\":\"~5B\",\"PPL\":\"28.0\"}\n",
      "{\"Model\":\"Shazeer et al. (2018) - Mesh Tensorflow\",\"#Param\":\"4.9B\",\"PPL\":\"24.0\"}\n",
      "{\"Model\":\"Baevski and Auli (2018) - Adaptive Input\",\"#Param\":\"0.46B\",\"PPL\":\"24.1\"}\n",
      "{\"Model\":\"Baevski and Auli (2018) - Adaptive Input\",\"#Param\":\"1.0B\",\"PPL\":\"23.7\"}\n",
      "{\"Model\":\"Ours - Transformer-XL Base\",\"#Param\":\"0.46B\",\"PPL\":\"23.5\"}\n",
      "{\"Model\":\"Ours - Transformer-XL Large\",\"#Param\":\"0.8B\",\"PPL\":\"21.8\"}\n",
      "\n",
      "The table compares various models in terms of their parameters and perplexity scores on the text8 dataset. Models include those by Shazeer et al., Chelba et al., and Jozefowicz et al., among others. The Transformer-XL models, both Base and Large, show improved perplexity scores compared to previous state-of-the-art models. The Transformer-XL Large achieves the lowest perplexity, indicating superior performance. The table highlights the efficiency and effectiveness of the Transformer-XL architecture in language modeling tasks.\n",
      "-----------\n",
      "• Finally, we deliberately separate the two weight matrices \\( W_{k,E} \\) and \\( W_{k,R} \\) for producing the content-based key vectors and location-based key vectors, respectively. Under the new parameterization, each term has an intuitive meaning: term (a) represents content-based addressing, term (b) captures a content-dependent positional bias, term (c) governs a global content bias, and term (d) encodes a global positional bias. \n",
      "\n",
      "In comparison, the formulation in Shaw et al. (2018) only has terms (a) and (b), dropping the two bias terms (c) and (d). Moreover, Shaw et al. (2018) merge the multiplication \\( W_{k,R} \\) into a single trainable matrix \\( R \\), which abandons the inductive bias built into the original sinusoid positional encoding (Vaswani et al., 2017). In contrast, our relative positional embedding \\( R \\) adapts the sinusoid formulation. As a benefit of the inductive bias, a model trained on a memory of a certain length can automatically generalize to a memory several times longer during evaluation.\n",
      "\n",
      "Equipping the recurrence mechanism with our proposed relative positional embedding, we finally arrive at the Transformer-XL architecture. For completeness, we summarize the computational procedure for an \\( N \\)-layer Transformer-XL with a single attention head here. For \\( n = 1, \\ldots, N \\):\n",
      "\n",
      "\\[\n",
      "\\begin{align*}\n",
      "\\tilde{h}^{n-1} &= [SG(m_1) \\; 0h_{-1}] \\\\\n",
      "q, k, v &= h^{n-1} W, \\tilde{h}^{n-1} W_E, W \\\\\n",
      "A_{i,j} &= q_{i}^{T} k_{j} + q_{r,i} W_R R_{ij} \\\\\n",
      "& \\quad + u_{k,r,j} v W_R R_{i-j} \\\\\n",
      "a &= \\text{Masked-Softmax}(A)v \\\\\n",
      "0 &= \\text{LayerNorm}(\\text{Linear}(a - 3) + h_{n-1}) \\\\\n",
      "h &= \\text{Positionwise-Feed-Forward}(0)\n",
      "\\end{align*}\n",
      "\\]\n",
      "\n",
      "with \\( h_0 \\) defined as the word embedding sequence. In addition, it is worth mentioning that a naive way to compute \\( A \\) requires computing \\( W_R R_{j} \\) for all pairs \\( (i, j) \\), whose cost is quadratic with respect to the sequence length. However, noticing that the value of \\( i - j \\) only ranges from zero to the sequence length, we show a simple computation procedure in Appendix B, which reduces the cost to be linear with respect to the sequence length.\n",
      "\n",
      "4 Experiments  \n",
      "4.1 Main Results  \n",
      "We apply Transformer-XL to a variety of datasets on both word-level and character-level language modeling tasks.\n",
      "-----------\n",
      "{\n",
      "  \"text\": \"Deep self-attention network. In particular, instead of computing the hidden states from scratch for each new segment, we reuse the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very long-term dependency becomes possible because information can be propagated through the recurrent connections. Meanwhile, passing information from the previous segment can also resolve the problem of context fragmentation. More importantly, we show the necessity of using relative positional encodings rather than absolute ones, in order to enable state reuse without causing temporal confusion. Hence, as an additional technical contribution, we introduce a simple but more effective relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training.\\n\\nTransformer-XL obtained strong results on five datasets, varying from word-level to character-level language modeling. Transformer-XL is also able to generate relatively coherent long text articles with thousands of tokens (see Appendix E), trained on only 100 million tokens.\\n\\nOur main technical contributions include introducing the notion of recurrence in a purely self-attentive model and deriving a novel positional encoding scheme. These two techniques form a complete set of solutions, as any one of them alone does not address the issue of fixed-length contexts.\\n\\nThe self-attention model, Transformer-XL, is the first model that achieves substantially better results than Recurrent Neural Networks (RNNs) on both character-level and word-level language modeling.\\n\\n2 Related Work\"\n",
      "}\n",
      "In the last few years, the field of language modeling has witnessed many significant advances, including but not limited to devising novel architectures to better encode the context (Bengio et al., 2003; Mikolov et al., 2010; Merity et al., 2016; Al-Rfou et al., 2018), improving regularization and optimization algorithms (Gal and Ghahramani, 2016), speeding up the Softmax computation (Grave et al., 2016a), and enriching the output distribution family (Yang et al., 2017).\n",
      "\n",
      "To capture the long-range context in language modeling, a line of work directly feeds a representation of the wider context into the network as an additional input. Existing works range from ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others that rely on document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017).\n",
      "\n",
      "More broadly, in generic sequence modeling, how to capture long-term dependency has been a long-standing research problem. From this perspective, since the ubiquitous adaptation of Long Short-Term Memory (LSTM) networks, many efforts have been spent on relieving the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signals (Trinh et al., 2018), augmented memory structures (Ke et al., 2018), and others that modify the internal architecture of Recurrent Neural Networks (RNNs) to ease the optimization (Wu et al., 2016; Li et al., 2018). Different from these approaches, our work is based on the Transformer architecture and shows that language modeling as a real-world task benefits from the ability to learn longer-term dependencies. \n",
      "\n",
      "3 Model\n",
      "{\n",
      "  \"text\": \"Given a corpus of tokens x = (x1,...,xT), the task of language modeling is to estimate the joint probability P(x), which is often auto-regressively factorized as P(x) = ∏_{t} P(xt | x<t). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x<t into a fixed-size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token.\\n\\n3.1 Vanilla Transformer Language Models\\nIn order to apply the Transformer or self-attention mechanism to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed-size representation. Given infinite memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer decoder, similar to a feed-forward neural network. However, this is usually infeasible with the limited resources available in practice. One feasible but crude approximation is to split the entire corpus into shorter segments of manageable length.\"\n",
      "}\n",
      "-----------\n",
      "{\"table_title\":\"Comparison with state-of-the-art results on enwik8\",\"table_number\":\"Table 2\",\"summary\":\"The following table contains a comparison of different models based on their parameter count and bits per character (bpc) performance on the enwik8 dataset. It highlights the performance of various models, including the Transformer-XL, against previous state-of-the-art models.\"}\n",
      "{\"model\":\"Ha et al. (2016) - LN HyperNetworks\",\"parameters\":\"27M\",\"bpc\":\"1.34\"}\n",
      "{\"model\":\"Chung et al. (2016) - LN HM-LSTM\",\"parameters\":\"35M\",\"bpc\":\"1.32\"}\n",
      "{\"model\":\"Zilly et al. (2016) - RHN\",\"parameters\":\"46M\",\"bpc\":\"1.27\"}\n",
      "{\"model\":\"Mujika et al. (2017) - FS-LSTM-4\",\"parameters\":\"47M\",\"bpc\":\"1.25\"}\n",
      "{\"model\":\"Krause et al. (2016) - Large mLSTM\",\"parameters\":\"46M\",\"bpc\":\"1.24\"}\n",
      "{\"model\":\"Knol (2017) - cmix v13\",\"parameters\":\"-\",\"bpc\":\"1.23\"}\n",
      "{\"model\":\"Al-Rfou et al. (2018) - 12L Transformer\",\"parameters\":\"44M\",\"bpc\":\"1.11\"}\n",
      "{\"model\":\"Ours - 12L Transformer-XL\",\"parameters\":\"41M\",\"bpc\":\"1.06\"}\n",
      "{\"model\":\"Al-Rfou et al. (2018) - 64L Transformer\",\"parameters\":\"235M\",\"bpc\":\"1.06\"}\n",
      "{\"model\":\"Ours - 18L Transformer-XL\",\"parameters\":\"88M\",\"bpc\":\"1.03\"}\n",
      "{\"model\":\"Ours - 24L Transformer-XL\",\"parameters\":\"277M\",\"bpc\":\"0.99\"}\n",
      "\n",
      "The table compares different models based on their parameter count and bits per character (bpc) performance on the enwik8 dataset. It includes models like LN HyperNetworks, HM-LSTM, RHN, FS-LSTM-4, Large mLSTM, cmix v13, and various Transformer models. The Transformer-XL models, particularly the 24-layer version, show significant improvements in bpc, achieving a value of 0.99, indicating superior performance compared to other models listed.\n",
      "-----------\n",
      "{\"Table Title\":\"Comparison with state-of-the-art results on WikiText-103\",\"Table Number\":\"Table 1\",\"Keywords\":\"Transformer-XL, state-of-the-art, language modeling, WikiText-103, perplexity, performance metrics\",\"Summary\":\"The following table contains a comparison of various models and their performance on the WikiText-103 dataset. It lists models, the number of parameters, and their perplexity scores. The Transformer-XL models, both standard and large, are highlighted for achieving superior results, demonstrating significant improvements over previous models.\"}\n",
      "{\"Model\":\"Grave et al. (2016b) - LSTM\",\"Param\":\"-\",\"PPL\":\"48.7\"}\n",
      "{\"Model\":\"Bai et al. (2018) - TCN\",\"Param\":\"-\",\"PPL\":\"45.2\"}\n",
      "{\"Model\":\"Dauphin et al. (2016) - GCNN-8\",\"Param\":\"-\",\"PPL\":\"44.9\"}\n",
      "{\"Model\":\"Grave et al. (2016b) - LSTM + Neural cache\",\"Param\":\"-\",\"PPL\":\"40.8\"}\n",
      "{\"Model\":\"Dauphin et al. (2016) - GCNN-14\",\"Param\":\"-\",\"PPL\":\"37.2\"}\n",
      "{\"Model\":\"Merity et al. (2018) - QRNN\",\"Param\":\"151M\",\"PPL\":\"33.0\"}\n",
      "{\"Model\":\"Rae et al. (2018) - Hebbian + Cache\",\"Param\":\"-\",\"PPL\":\"29.9\"}\n",
      "{\"Model\":\"Ours - Transformer-XL Standard\",\"Param\":\"151M\",\"PPL\":\"24.0\"}\n",
      "{\"Model\":\"Baevski and Auli (2018) - Adaptive Input\",\"Param\":\"247M\",\"PPL\":\"20.5\"}\n",
      "{\"Model\":\"Ours - Transformer-XL Large\",\"Param\":\"257M\",\"PPL\":\"18.3\"}\n",
      "\n",
      "The table compares various models based on their performance on the WikiText-103 dataset. It includes models such as LSTM, TCN, GCNN, and QRNN, listing the number of parameters and their perplexity scores. The Transformer-XL models, both standard and large, are noted for achieving lower perplexity scores, indicating superior performance. The table highlights the advancements made by Transformer-XL in language modeling tasks.\n",
      "-----------\n",
      "Table 2: Comparison with state-of-the-art results on enwik8.\n",
      "\n",
      "This section provides a comparison with state-of-the-art systems, including WikiText-103 (Merity et al., 2016), enwik8 (LLC, 2009), text8 (LLC, 2009), One Billion Word (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012). \n",
      "\n",
      "WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103 million training tokens from 28,000 articles, with an average length of 3,600 tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1,600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018; Grave et al., 2016a). As shown in Table 1, Transformer-XL reduces the previous state-of-the-art (SOTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the Transformer-XL architecture.\n",
      "\n",
      "The dataset enwik8 contains 100 million bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in Table 2. Under the model size constraint, the 12-layer Transformer-XL achieves a new SOTA result, outperforming the 12-layer vanilla Transformer from Al-Rfou et al. (2018) by 0.05, while both Transformer variants have a large margin over conventional RNN-based models. Notably, our 12-layer architecture achieves the same result as the 64-layer network from Al-Rfou et al. (2018), using only 17% of the parameter budget. In order to see whether better performances can be obtained by increasing the model size, we train 18-layer and 24-layer Transformer-XLs with increased model sizes. With the attention length set to 784 during training and 3,800 during evaluation, we obtained a new SOTA result, and our method is the first to break through 1.0 on widely-studied character-level benchmarks. Different from Al-Rfou et al. (2018), Transformer-XL does not need any auxiliary losses, and thus all benefits are credited to a better architecture.\n",
      "\n",
      "Similar to but different from enwik8, text8 contains 100 million processed Wikipedia characters created by lowercasing the text and removing any character other than the 26 letters a through z, and space. Due to the similarity, we simply adapt the best model and the same hyperparameters from enwik8 to text8 without further tuning. The comparison with previous methods is summarized in Table 3. Again, Transformer-XL achieves the new SOTA result with a clear margin.\n",
      "-----------\n",
      "Again, each row of D is simply a left-shift version of d. Hence, the main computation cost comes from the matrix-vector multiplication d = [Qv], which is not expensive anymore.\n",
      "\n",
      "C. Details About RECL\n",
      "\n",
      "(b) Transformer-XL vs. Baseline  \n",
      "(a) Transformer-XL vs. RNNs  \n",
      "\n",
      "Figure 3: Visualizing unnormalized relative perplexity gains with r = [insert value or context if applicable].\n",
      "-----------\n",
      "Table 5: Comparison with state-of-the-art results on the Penn Treebank indicates using two-step fine-tuning. \n",
      "\n",
      "The One Billion Word dataset does not preserve any long-term dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4. Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model state-of-the-art (SOTA) from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting that the advantage of Transformer-XL is generalizable to modeling short sequences. \n",
      "\n",
      "We also report the results on word-level Penn Treebank in Table 5. Similar to AWD-LSTM (Merity et al., 2017), we apply variational dropout and weight averaging to Transformer-XL. With proper regularization, Transformer-XL achieves a new SOTA result among models without two-step fine-tuning. The Penn Treebank dataset has only 1 million training tokens, which implies that Transformer-XL also generalizes well even on small datasets.\n",
      "\n",
      "4.2 Ablation Study\n",
      "We conduct two sets of ablation studies to examine the effects of two proposed techniques used in Transformer-XL: the recurrence mechanism and the new positional encoding scheme. \n",
      "\n",
      "The first study is performed on the WikiText-103 dataset, which requires modeling long-term dependency. The results are reported in Table 6. Among the compared encoding schemes, the method by Shaw et al. (2018) is relative, while those by Vaswani et al. (2017) and Al-Rfou et al. (2018) are absolute. \"Full\" and \"half\" losses refer to applying a cross-entropy loss to all or the recent half of the positions in the segment. We found that absolute encodings only work well with half losses because half losses exclude positions with very short attention lengths during training for better generalization. Table 6 shows that both the recurrence mechanism and our encoding scheme are necessary to achieve the best performance, as well as to generalize to longer attention sequences during evaluation time. Although the backpropagation length during training is only 128, with the two techniques, the attention length can be increased to 640 at test time. In the standard setting with 151 million parameters, the perplexity decreases as the attention length increases.\n",
      "\n",
      "Since the recurrence mechanism incurs additional memory costs, we also compare Transformer-XL with baselines under the same GPU memory constraints. As shown in Table 10 in Appendix A, despite using a shorter backpropagation length, Transformer-XL remains superior to the baselines.\n",
      "\n",
      "The second study targets isolating the effects of resolving the context fragmentation problem from the benefit of capturing longer context length. In order to achieve this goal, we deliberately choose a dataset that does not require long-term dependency, so that any improvement from establishing the recurrence can be attributed to solving the context fragmentation. Specifically, we perform this controlled experiment on the One Billion Word dataset, which can only benefit from removing the context fragmentation. We train a 20-layer Transformer-XL with approximately 0.3 billion parameters for 400,000 steps. As shown in Table 7, using segment-level recurrence substantially improves performance even when long-term dependency is not needed, which is consistent with our previous discussion that the recurrence mechanism resolves the context fragmentation problem. Moreover, our relative positional encodings are also superior to those of Shaw et al. (2018) on short sequences.\n",
      "\n",
      "4.3 Relative Effective Context Length\n",
      "Khandelwal et al. (2018) proposed a method to evaluate the Effective Context Length (ECL) of a sequence model. ECL is defined as the longest length to which increasing the context span would lead to a gain greater than a specified threshold. However, ECL ignores the fact that it is harder to achieve improvement when a model already achieves a lower perplexity using only a shorter context, and thus it is not suitable for a fair comparison among multiple models. We instead propose a new metric.\n",
      "\n",
      "Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128 million parameters). \n",
      "\n",
      "† indicates that the corresponding row is reduced to the same setting as the Transformer network in Al-Rfou et al. (2018), except that two auxiliary losses are not implemented in our experiments. \n",
      "\n",
      "“PPL init” refers to using the same length as training. \n",
      "\n",
      "“PPL best” indicates the perplexity obtained by using the optimal length. \n",
      "\n",
      "“Attn Len” is the shortest possible attention length during evaluation to achieve the corresponding result (PPL best). \n",
      "\n",
      "Increasing the attention length during evaluation improves performance only when our positional encoding is used. The \"Transformer-XL (151 million)\" setting uses a standard parameter budget as previous work (Merity et al., 2018), where we observe a similar effect when increasing the attention length during evaluation.\n",
      "-----------\n",
      "{\"Table Title\":\"Ablation Study with Memory Constraints\",\"Table Number\":\"Table 10\",\"Keywords\":\"Ablation Study, Memory Constraints, Transformer-XL, Recurrence, Encoding, Loss\",\"Summary\":\"The following table contains an ablation study comparing different configurations of the Transformer-XL model under the same GPU memory constraints. It evaluates the impact of backpropagation length, recurrence, encoding, and loss on perplexity and attention length. The table demonstrates that Transformer-XL outperforms the baseline even with a shorter backpropagation length.\"}\n",
      "{\"Backprop Len\":128,\"Recurrence\":\"Yes\",\"Encoding\":\"Ours\",\"Loss\":\"Full\",\"pplx best\":26.77,\"pplx init\":27.02,\"Attn Len\":500}\n",
      "{\"Backprop Len\":128,\"Recurrence\":\"Yes\",\"Encoding\":\"Ours\",\"Loss\":\"Partial\",\"pplx best\":28.33,\"pplx init\":28.69,\"Attn Len\":460}\n",
      "{\"Backprop Len\":176,\"Recurrence\":\"No\",\"Encoding\":\"Ours\",\"Loss\":\"Full\",\"pplx best\":27.98,\"pplx init\":28.43,\"Attn Len\":400}\n",
      "{\"Backprop Len\":172,\"Recurrence\":\"No\",\"Encoding\":\"Ours\",\"Loss\":\"Partial\",\"pplx best\":28.83,\"pplx init\":28.83,\"Attn Len\":120}\n",
      "\n",
      "The table titled \"Ablation Study with Memory Constraints\" is labeled as Table 10. It presents an ablation study comparing different configurations of the Transformer-XL model under the same GPU memory constraints. The table evaluates the impact of backpropagation length, recurrence, encoding, and loss on perplexity and attention length. It demonstrates that Transformer-XL outperforms the baseline even with a shorter backpropagation length. The first configuration has a backpropagation length of 128, uses recurrence, employs our encoding method, and has a full loss with a perplexity best of 26.77 and an attention length of 500. The second configuration also has a backpropagation length of 128, uses recurrence, employs our encoding method, and has a partial loss with a perplexity best of 28.33 and an attention length of 460. The third configuration has a backpropagation length of 176, does not use recurrence, employs our encoding method, and has a full loss with a perplexity best of 27.98 and an attention length of 400. The fourth configuration has a backpropagation length of 172, does not use recurrence, employs our encoding method, and has a partial loss with a perplexity best of 28.83 and an attention length of 120.\n",
      "-----------\n",
      "{\"Model\":\"Cooijmans et al. (2016) - BN-LSTM\",\"#Param\":\"-\",\"bpc\":\"1.36\"}\n",
      "{\"Model\":\"Chung et al. (2016) - LN HM-LSTM\",\"#Param\":\"35M\",\"bpc\":\"1.29\"}\n",
      "{\"Model\":\"Zilly et al. (2016) - RHN\",\"#Param\":\"45M\",\"bpc\":\"1.27\"}\n",
      "{\"Model\":\"Krause et al. (2016) - Large mLSTM\",\"#Param\":\"45M\",\"bpc\":\"1.27\"}\n",
      "{\"Model\":\"Al-Rfou et al. (2018) - 12L Transformer\",\"#Param\":\"44M\",\"bpc\":\"1.18\"}\n",
      "{\"Model\":\"Al-Rfou et al. (2018) - 64L Transformer\",\"#Param\":\"235M\",\"bpc\":\"1.13\"}\n",
      "{\"Model\":\"Ours - 24L Transformer-XL\",\"#Param\":\"277M\",\"bpc\":\"1.08\"}\n",
      "\n",
      "The table compares various models based on their parameters and bits per character (bpc) performance. It includes models like BN-LSTM, LN HM-LSTM, RHN, and different versions of the Transformer model. The \"Ours - 24L Transformer-XL\" model shows the best performance with a bpc of 1.08, indicating its efficiency in language modeling tasks.\n",
      "-----------\n",
      "Table 7: Ablation study on the One Billion Word dataset, which is a dataset without long-term dependency.\n",
      "\n",
      "Table 9: Slowdown in terms of running time during evaluation. Evaluation is based on per-token time on one Graphics Processing Unit (GPU).\n",
      "\n",
      "Table 8: Relative Effective Context Length (RECL) comparison. See the text for the definition of RECL and the parameter r. The first three models and the last four models are compared as two model groups when we calculate RECL (RECL is computed on a model group rather than a single model). Each group has the same parameter budget.\n",
      "\n",
      "RECL, called Relative Effective Context Length, is defined on a model group instead of a single model, and the gain of a long context is measured by the relative improvement over the best short context model. As such, the model group shares the same baseline to enable fair comparison. RECL also has a parameter r, which means constraining the comparison on top-r hard examples. See Appendix C for more details about RECL.\n",
      "\n",
      "As shown in Table 8, Transformer-XL manages to model dependencies of 900 words long on average with r = 0.1. The RECL of Transformer-XL is 80% and 450% longer than that of recurrent networks and the Transformer model, respectively. Both the recurrence mechanism and our positional encodings contribute to a longer RECL. This further substantiates our argument that Transformer-XL is able to model longer-term dependencies.\n",
      "-----------\n",
      "{\"table_title\":\"RoFormer: Enhanced Transformer with Rotary Position Embedding\",\"table_number\":null,\"keywords\":\"RoFormer,Zhuiyi Technology,Shenzhen,Natural Language Processing,Position Embedding\",\"summary\":\"The following table contains the names, affiliations, locations, and email addresses of the authors of the paper titled 'RoFormer: Enhanced Transformer with Rotary Position Embedding'. All authors are affiliated with Zhuiyi Technology Co., Ltd., located in Shenzhen, and their contact details are provided.\"}\n",
      "{\"name\":\"Jianlin Su\",\"affiliation\":\"Zhuiyi Technology Co., Ltd.\",\"location\":\"Shenzhen\",\"email\":\"bojonesu@wezhuiyi.com\"}\n",
      "{\"name\":\"Yu Lu\",\"affiliation\":\"Zhuiyi Technology Co., Ltd.\",\"location\":\"Shenzhen\",\"email\":\"julianlu@wezhuiyi.com\"}\n",
      "{\"name\":\"Shengfeng Pan\",\"affiliation\":\"Zhuiyi Technology Co., Ltd.\",\"location\":\"Shenzhen\",\"email\":\"nickpan@wezhuiyi.com\"}\n",
      "{\"name\":\"Ahmed Murtadha\",\"affiliation\":\"Zhuiyi Technology Co., Ltd.\",\"location\":\"Shenzhen\",\"email\":\"mengjiayi@wezhuiyi.com\"}\n",
      "{\"name\":\"Bo Wen\",\"affiliation\":\"Zhuiyi Technology Co., Ltd.\",\"location\":\"Shenzhen\",\"email\":\"brucewen@wezhuiyi.com\"}\n",
      "{\"name\":\"Yunfeng Liu\",\"affiliation\":\"Zhuiyi Technology Co., Ltd.\",\"location\":\"Shenzhen\",\"email\":\"glenliu@wezhuiyi.com\"}\n",
      "\n",
      "Jianlin Su is affiliated with Zhuiyi Technology Co., Ltd., located in Shenzhen, and can be contacted at bojonesu@wezhuiyi.com. Yu Lu, also from Zhuiyi Technology Co., Ltd. in Shenzhen, can be reached at julianlu@wezhuiyi.com. Shengfeng Pan, another contributor from Zhuiyi Technology Co., Ltd. in Shenzhen, is available at nickpan@wezhuiyi.com. Ahmed Murtadha, affiliated with Zhuiyi Technology Co., Ltd. in Shenzhen, can be contacted at mengjiayi@wezhuiyi.com. Bo Wen, also from Zhuiyi Technology Co., Ltd. in Shenzhen, can be reached at brucewen@wezhuiyi.com. Yunfeng Liu, a contributor from Zhuiyi Technology Co., Ltd. in Shenzhen, is available at glenliu@wezhuiyi.com.\n",
      "-----------\n",
      "4.4 Generated Text  \n",
      "Trained only on WikiText-103, which is a medium-sized dataset, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry-picking, despite minor flaws. Please refer to Appendix E for samples.\n",
      "\n",
      "4.5 Evaluation Speed  \n",
      "Finally, we compare the evaluation speed of our model with the vanilla Transformer model (Rami Al-Rfou et al., 2018). As shown in Table 9, due to the state reuse scheme, Transformer-XL achieves up to a 1,874 times speedup during evaluation.\n",
      "\n",
      "5 Conclusions  \n",
      "Transformer-XL obtains strong perplexity results, models longer-term dependencies than Recurrent Neural Networks (RNNs) and the Transformer model, achieves substantial speedup during evaluation, and is able to generate coherent text articles. We envision interesting applications of Transformer-XL in the fields of text generation, unsupervised feature learning, image modeling, and speech modeling.\n",
      "\n",
      "Acknowledgments  \n",
      "Zhongdong (ZD) and Yiming (YY) were supported in part by the National Science Foundation (NSF) under the grant IIS-1546329 and by the Department of Energy (DOE) Office of Science under the grant ASCR #KJ040201. Zeynep (ZY) and Rami (RS) were supported in part by the Office of Naval Research grant N000141812861, the NSF grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship.\n",
      "\n",
      "References  \n",
      "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444.  \n",
      "Alexei Baevski and Michael Auli. 2018. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853.  \n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.  \n",
      "Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271.\n",
      "Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3 (February): 1137–1155.\n",
      "\n",
      "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Philipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005.\n",
      "\n",
      "Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. 2016. Hierarchical multiscale recurrent neural networks. arXiv preprint arXiv:1609.01704.\n",
      "\n",
      "Tim Cooijmans, Nicolas Ballas, César Laurent, Çağlar Gülçehre, and Aaron Courville. 2016. Recurrent batch normalization. arXiv preprint arXiv:1603.09025.\n",
      "\n",
      "Andrew M. Dai and Quoc V. Le. 2015. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems, pages 3079-3087.\n",
      "\n",
      "Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2016. Language modeling with gated convolutional networks. arXiv preprint arXiv:1612.08083.\n",
      "\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
      "\n",
      "Adji B. Dieng, Chong Wang, Jianfeng Gao, and John Paisley. 2016. TopicRNN: A recurrent neural network with long-range semantic dependency. arXiv preprint arXiv:1611.01702.\n",
      "\n",
      "Yarin Gal and Zoubin Ghahramani. 2016. A theoretically grounded application of dropout in recurrent neural networks. In Advances in Neural Information Processing Systems, pages 1019-1027.\n",
      "\n",
      "Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou. 2016a. Efficient softmax approximation for GPUs. arXiv preprint arXiv:1609.04309.\n",
      "\n",
      "Edouard Grave, Armand Joulin, and Nicolas Usunier. 2016b. Improving neural language models with a continuous cache. arXiv preprint arXiv:1612.04426.\n",
      "\n",
      "Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint.\n"
     ]
    }
   ],
   "source": [
    "for result in context.search.results:\n",
    "    print('-----------')\n",
    "    print(result.suggested_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafaf7b0",
   "metadata": {},
   "source": [
    "**Creating citable sources**\n",
    "unpacking the GX response to convert it into a format which the citing library respects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7784a37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n",
      "dict_keys(['text', 'uuid', 'render_name', 'source_data'])\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "citable_sources = []\n",
    "\n",
    "for result in context.search.results:\n",
    "    text = result.text.strip()\n",
    "    if not text:\n",
    "        continue  # Skip empty chunks\n",
    "\n",
    "    page_num = result.bounding_boxes[0].page_number\n",
    "    page_url = f\"{result.source_url}#page={page_num}\"\n",
    "\n",
    "    citable_sources.append({\n",
    "        \"text\": text,\n",
    "        \"uuid\": str(uuid.uuid4()),\n",
    "        \"render_name\": result.file_name,\n",
    "        \"source_data\": {\n",
    "            \"url\": page_url,\n",
    "            \"page_number\": page_num\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Optional: preview\n",
    "for source in citable_sources:\n",
    "    print(source.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c28a04",
   "metadata": {},
   "source": [
    "**Generating Cited Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc3346c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .page {\n",
       "            padding: 2rem;\n",
       "            font-family: Arial, sans-serif;\n",
       "            line-height: 1.6;\n",
       "            background-color: #fefefe;\n",
       "            border: 1px solid #ddd;\n",
       "            border-radius: 8px;\n",
       "            max-width: 800px;\n",
       "            margin: 2rem auto;\n",
       "            box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
       "            white-space: normal;\n",
       "        }\n",
       "\n",
       "        intextcitation {\n",
       "            display: inline-block;\n",
       "            background-color: #e0e0ff;\n",
       "            color: #333;\n",
       "            border: 1px solid #aaa;\n",
       "            border-radius: 5px;\n",
       "            padding: 2px 8px;\n",
       "            margin: 0 3px;\n",
       "            font-size: 0.85em;\n",
       "            font-family: monospace;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        intextcitation:hover {\n",
       "            background-color: #d0d0f0;\n",
       "        }\n",
       "\n",
       "        intextcitation::before {\n",
       "            content: attr(renderName);\n",
       "        }\n",
       "\n",
       "        .final-answer {\n",
       "            color: black;\n",
       "        }\n",
       "    </style>\n",
       "\n",
       "    <div class=\"page\">\n",
       "        Rationale: The original Transformer architecture, introduced by Vaswani et al. in 2017, is a model that utilizes self-attention mechanisms to process sequences of data, which allows it to capture dependencies between elements regardless of their distance in the sequence. It uses absolute positional encodings to maintain the order of the sequence \n",
       "<InTextCitation chunkId=\"83358da0-baa8-45f5-afae-4706a91de345\" renderName=\"rope_roformer.pdf\" url=\"https%3A//upload.groundx.ai/file/974d725e-e804-49f4-9792-de162e50a15c/6329b6ff-00d2-4bf1-8646-0af3f6b089ec.pdf%23page%3D2\" page_number=\"2\"></InTextCitation>.\n",
       "\n",
       "The RoFormer is an enhanced version of the Transformer that incorporates Rotary Position Embedding (ROPE). This method encodes absolute positions with a rotation matrix and incorporates explicit relative position dependencies in the self-attention formulation. This approach allows for flexibility in sequence length and improves the modeling of inter-token dependencies with increasing relative distances. The RoFormer has shown improved performance on long text classification tasks compared to its alternatives \n",
       "<InTextCitation chunkId=\"83358da0-baa8-45f5-afae-4706a91de345\" renderName=\"rope_roformer.pdf\" url=\"https%3A//upload.groundx.ai/file/974d725e-e804-49f4-9792-de162e50a15c/6329b6ff-00d2-4bf1-8646-0af3f6b089ec.pdf%23page%3D2\" page_number=\"2\"></InTextCitation>.\n",
       "\n",
       "Transformer-XL, on the other hand, introduces a recurrence mechanism in the self-attention model, allowing it to reuse hidden states from previous segments as memory for the current segment. This enables the model to capture very long-term dependencies and resolve context fragmentation issues. Transformer-XL also uses a novel relative positional encoding scheme, which generalizes to longer attention lengths than those observed during training. This architecture has demonstrated superior performance in modeling long-term dependencies compared to both RNNs and the original Transformer \n",
       "<InTextCitation chunkId=\"ae86cba5-7ecf-4b79-9152-7a9cfb384212\" renderName=\"transformer_xl.pdf\" url=\"https%3A//upload.groundx.ai/file/974d725e-e804-49f4-9792-de162e50a15c/b75328a5-f4a9-4481-bb95-1b0c177f5204.pdf%23page%3D2\" page_number=\"2\"></InTextCitation>.\n",
       "\n",
       "Final Answer:<span class=\"final-answer\"> The original Transformer uses absolute positional encodings and self-attention to process sequences. RoFormer enhances this by using Rotary Position Embedding (ROPE) to incorporate relative position dependencies, improving performance on long texts. Transformer-XL introduces a recurrence mechanism to reuse hidden states, capturing long-term dependencies and resolving context fragmentation, with a novel relative positional encoding scheme.</span>\n",
       "    </div>\n",
       "\n",
       "    <script>\n",
       "        setTimeout(() => {\n",
       "            document.querySelectorAll(\"intextcitation\").forEach(el => {\n",
       "                el.addEventListener(\"click\", () => {\n",
       "                    const encodedUrl = el.getAttribute(\"url\");\n",
       "                    if (encodedUrl) {\n",
       "                        const decodedUrl = decodeURIComponent(encodedUrl);\n",
       "                        window.open(decodedUrl, \"_blank\");\n",
       "                    }\n",
       "                });\n",
       "            });\n",
       "        }, 0);\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await generate_cited_response(\n",
    "    chunks=citable_sources,\n",
    "    system_prompt=(\n",
    "        \"You are a helpful assistant. Use the provided excerpts to fully analyze and answer the user's question. \"\n",
    "        \"You must rely only on information from the excerpts, and cite every excerpt you used using the $REF: ID$ format. \"\n",
    "        \"If any information is irrelevant, do not cite it or mention it. Think carefully and show clear reasoning.\"\n",
    "        \"provide your answer as a \\\"rationale: \\\" which includes in-text citations, and \\\"final answer: \\\" which is a concise answer to the question. \"\n",
    "        \"the final answer should be separated by the rationale by a paragraph break. The rationale should not include newlines.\"\n",
    "    ),\n",
    "    query=query\n",
    ")\n",
    "\n",
    "display(HTML(render_citation_with_clickable_custom_tags(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ffea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groundx-community",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
